Week 1

What is an Operating System?

	The software layer between user applications and hardware.

		Turns ugly hardware into beautiful abstractions (provides services)

		Serves as a resource  manager

			Allows proper use of resources (data, handware, software)

		Serves as a control program (protection)

			Controls execution of user programs to prevent error and improper use of computer

		
	Three core abstractions and resource management services:

		Processes & threads
		Memory management
		File & I/O systems

	Storage Hierachy 层次

		Processor registers (寄存器), main memory 主存储器, and auxiliary memory 辅存储器 form a rudimentary 基本的 memory hierarchy

		The hierarchy can be classified according to memory speed, cost and volatility 波动性

		Caches 快速缓冲贮存区 can be installed to hide performance differences when there is a large access-time gap between two levels

	Storage Structure

		Main memory (DRAM) store programs and data during execution:

			DRAM 随机存取存储器 cannot store these permanently 永久的 because it is too small & it is a volatile storage device

			Forms a large array of bytes (1 byte = 8 bits) of memory, each with its own address

				We say that main memory is byte-addressable

	Caching 缓存

		When the processor accesses info at some level of the storage hierachy,
		that infomay be copied tp a cche memory closer to the processor, on a
		temporary basis.

			A cache is smaller and costlier

		Because caches have limited sizes, cache management is an important design problem

			Coherency 一致

	Concurrency

		Every modern computer is a multiprocessor

			CPU and device controllers can execute concurrently,competing for memory cycles

			A memory controller synchronizes access to shared memory

			Interrupts allow device controllers to signal the CPU that
			some event has occurred (e.g. disk I/O complete, network
			packet arrived, etc.)

				Generated by a hardware device

			Interrupts are also used to signal errors (e.g. division by zero)
			or requests for OS service from a user program (a system call)

				These types of interrupts are called traps or exceptions

			An Operating System is an event-driven program

	C Programming & Memory
		A variable in a C program is a symbolic name for a data item, stored in memory

			 The type of the variable indicates how much storage (how many bytes) it needs
			
			 Type also determines alignment requirements
			
			 The address of the variable is an index into the big array of
			memory words where the data item is stored
			
			 The value of the variable is the actual contents of memory at
			that location
			
			 A pointer type variable is just a data item whose contents are
			a memory location (usually the address of another var)

		char data type is 1 byte in size
		int data type is 1 word in size (32 bits for most current architectures)
			occupies 4 bytes,should be word-aligned
		pointer types are all 1 word in size

Processes and Threads 执行绪
	
	Part 1: The Process Concept

		 Process = job / unit of work
		 Process = a program in execution
		A process contains all state of program in execution
			An address space
			
			Set of OS resourcess
				Opend files network connections ...

			Set of general-purpose registers with current values

		A process is named by its process ID (PID)

	Process Data Structures

		How does the OS represent a process in the kernel?

			At any time, there are many processes in the system,
			each in its own particular state

			The OS data structure representing each process is
			called the Process Control Block (PCB)

			The PCB contains all of the info about a process

			The PCB also is where the OS keeps all of a process’
			hardware execution state (PC, SP, regs, etc.) when the
			process is not running

	Process Control Block
		
		 Generally includes:
			 Process state (ready, running, blocked …)
			 Program counter: address of the next instruction
			 CPU registers: must be saved at an interrupt
			 CPU scheduling information: process priority
			 Memory management info: page tables
			 I/O status information: list of open files


	Process states & state changes

		The OS manages processes by keepng track of their state

	State Queues

		How does the OS keep track of processes?

			The OS maintains a collection of queues that represent
			the state of all processes in the system

			Typically, the OS has one queue for each state (Ready,waiting..

			Each PCB is queued on a state queue according its current state

			As a process changes state, its PCB is unlinked from
			one queue and linked into another

	PCBs and State Queues

		PCBs are data structures dynamically allocated in OS memory

		When a process is created, the OS allocates
		a PCB for it, initializes it, and places it on the Ready queue

		As the process computes, does I/O, etc., its
		PCB moves from one queue to another
		
		When the process terminates, its PCB is deallocated 解除配置

	What is a Context Switch?

		Context switch: switch the CPU to another
		process, saving the state of the old process
		and loading the saved state for the new
		process

		Context switch time is pure overhead, so some
		systems offer specific hardware support

		A performance bottleneck, so new structures
		(threads) are being used to avoid it

	Operations on Processes

		Processes execute concurrently and must be
		created and deleted dynamically

		 Process creation:
			 System Initialization
			 A running process
			 A user request
			 Initialization of a batch job

		 Process termination:
			 When a process finishes executing last statement
			 When a parent causes the termination of a child
			 An Error occurred



	Process Creation

		A process is created by another process

			Parent is creator, child is created

				 In Linux, the parent is the “PPID” field of “ps –f”

		In some systems, the parent defines (or donates)
		resources and privileges 特权 for its children

			Unix: Process User ID is inherited – children of your shell
			execute with your privileges

		After creating a child, the parent may either wait for
		it to finish its task or continue in parallel (or both)

	Process Creation: Unix (1)

		In Unix, processes are created using fork()
			int fork()

		fork()
			 Creates a new address space
			
			 Initializes the address space with a copy of the entire
			contents of the address space of the parent
			
			 Initializes the kernel resources to point to the resources
			used by parent (e.g., open files)

		Fork returns twice
			 Returns the child’s PID to the parent, “0” to the child

		Why fork()?

			Very useful when the child…
				 Is cooperating with the parent
				 Relies upon the parent’s data to accomplish its task

		Wait():

			while (1) {
				char *cmd = read_command();
				int child_pid = fork();
				if (child_pid == 0) {
					exec(cmd);
				} else {
					wait(child_pid);
				}
			}
		Wait for child.

	Parallel Programs

		Recall our Web server example that forks off copies of itself
		to handle multiple simultaneous requests
			 Or any parallel program that executes on a multiprocessor

		To execute these programs we need to
			 Create several processes that execute in parallel
			 Create shared memory for processes to share data
			 Have the OS schedule these processes in parallel 

		This situation is very inefficient
			 Space: PCB, page tables, etc.
			 Time: create data structures, fork and copy addr space, etc.


	Rethinking Process

		What is similar in these cooperating processes?
			 They all share the same code and data (address space)
			 They all share the same privileges
			 They all share the same resources (files, sockets, etc.)

		What don’t they share?
			 Each has its own execution state: PC, SP, and registers

		PC : program counter --> Program counter is a register which holds the address of next instruction when the current instruction is under execution
		SP : stack pointer --> tack pointer is a pointer which points to the top element of the stack


		Key idea: Why don’t we separate the concept of a process from its execution state?
			
			 Process: address space, privileges, resources, etc.
			 Execution state: PC, SP, registers

		Exec state also called thread of control, or thread

	Threads

		Separate the concepts of process and threads:

			 The thread defines a sequential execution stream within a process (PC, SP, registers)
			
			 The process defines the address space and general process
			attributes (everything but threads of execution)

		A thread is bound to a single process:

			Processes, however, can have multiple threads
			Every process has at least one thread

		Processes are the containers in which threads execute
			 Processes become static, threads are the dynamic entities

		For web server:

			Using fork() to create new processes to handle
			requests in parallel is overkill for such a simple task

			Instead, we can create a new thread for each request

				web_server() {
					while (1) {
						int sock = accept();
						thread_fork(handle_request, sock);
					}
				}
					
				handle_request(int sock) {
					Process request
					close(sock);
				}

		Thread Interface:

			 pthread_create(pthread_t *tid, pthread_attr_t attr, void
			*(*start_routine)(void *), void *arg)
				 Create a new thread of control
				 New thread id returned in tid, new thread starts executing in
				start_routine with argument arg
			
			 pthread_join(pthread_t tid)
				 Wait for tid to exit
			
			 pthread_cancel(pthread_t tid)
				 Destroy tid
			
			 pthread_exit()
				 Terminate the calling thread

		Thread Scheduling

			The thread scheduler determines when a thread runs

			It uses queues to keep track of what threads are doing
				 Just like the OS and processes
				 But it is implemented at user-level in a library

			Run queue: Threads currently running (usually one)
			 Ready queue: Threads ready to run

		Threads Summary

			The operating system is a large multithreaded program
				 Each process executes as a thread within the OS

			 Multithreading is also very useful for applications
				 Efficient multithreading requires fast primitives 原型
				 Processes are too heavyweight

			Solution is to separate threads from processes

		Cooperating Processes

			A process is independent if it cannot affect or
			be affected by the other processes executing
			in the system

			No data sharing ==> process is independent

			 process is cooperating if it is not independent

			Cooperating processes must be able to
			communicate with each other and to
			synchronize their actions

		Interprocess Communication:	

			Cooperating processes need to exchange information, using either
				 Shared memory (e.g. fork())
				 Message passing
			

			Message passing models
				 send(P, msg) – send msg to process P
				 receive(Q, msg) – receive msg from process Q


Week 2
	
	Bootstrapping

		Hardware stores small program in non-volatile memory

			BIOS – Basic Input Output System

			Knows how to access simple hardware devices

				Disk, keyboard, display

		 When power is first supplied, this program executes

		 What does it do?

		 	Checks that RAM, keyboard, and basic devices are installed and
			functioning correctly

			Scans buses to detect 发现 attached 附属的 devices and configures 配置 new ones

			Determines boot device 启动设备 (tries list of devices in order)

				always be hard disk

			Reads first sector 部门 from boot device and executes it (bootloader)

				Bootloader是嵌入式系统在加电后执行的第一段代码，在它完成CPU和相关硬件的初始化之后，再将操作系统映像或固化的嵌入式应用程序装在到内存中然后跳转到操作系统所在的空间，启动操作系统运行

			Bootloader reads partition table, finds active partition, reads secondary bootloader

			Secondary bootloader reads OS into memory and executes it

		Operating System Startup

			Machine starts in system mode, so kernel code can execute immediately

			OS initialization:
				
				 Initialize internal data structures
					 Machine dependent operations are typically done first

						Programs that run on a variety of different types of computers are called machine independent.
				
				 Create first process
				
				 Switch mode to user and start running first process
				
				 Wait for something to happen
					 OS is entirely driven by external events

		Process Creation : Unix(2)

			How do we actually start a new program?
				exec()

			exec():

				 Stops the current process
				
				 Loads the program “prog” into the process’ address space
				
				 Initializes hardware context and args for the new program
				
				 Places the PCB onto the ready queue
				
				 Note: It does not create a new process

		Requesting OS Services

			Operating System and user programs are isolated from each other

			But OS provides service to user programs…

		Boundary Crossings

			Getting to kernel mode:

				Boot time 引导时间 (not really a crossing, starts in kernel)

				Explicit system call – request for service by application

				Hardware interrupt 硬件中断
				
				Software trap or exception

					A trap is a kind of exceptions, whose main purpose is for debugging (eg. notify the debugger that an instruction has been reached)
				
				Hardware has table of “Interrupt service routines”

					In computer systems programming, an interrupt handler, also known as an interrupt service routine or ISR, is a callback function in microcontroller firmware, an operating system, or a device driver whose execution is triggered by the reception of an interrupt


			Kernel to user:

				Jumps to next application instruction

		System Calls for Process Management

			Process management:

				Call ---> description

					pid = fork() --> Create a child process identical to the parent

					pid = waitpid(pid,&staloc,options) --> Wait for a child to terminate

					s = execve(name,argv,environp) --> replace a process' core image

					exit(status) --> terminate process execution and return status

		System Calls for File Management

			File management 

				Call --> description

					fd = open(file,how,...) --> opend a file for reading writing or both

					s = close(fd) --> close an open file

					n = read(fd,buffer,nbytes) --> read data from file to a buffer

					n = write(fd,buffer,nbytes) --> write data from buffer to a file

					position = lseek(fd,offset,whence) --> move the file pointer

					s = stat(name, &buf) = Get a file's status information

		System call Interface

			User program calls C library function with arguments
			
			C library function arranges to pass arguments to OS,
			including a system call identifier

			Executes special instruction to trap to system mode

			Syscall handler figures out which system call is needed and calls a routine for that operation

			How does this differ from a normal C language
			function call? Why is it done this way?

				Extra level of indirection through system call handler, rather
				than direct control flow to called function

				Hardware support is needed to enforce separation of
				userspace and kernel

		System call operation

			Kernel must verify arguments that it is passed

			A fixed number of arguments can be passed in registers

				 Often pass the address of a user buffer
				containing data (e.g., for write())
				
				 Kernel must copy data from user space into its
				own buffers

			Result of system call is returned in register


	Intro to Synchronization:

		Deposit and withdrwa example

		What went wrong:

			Two concurrent threads manipulated a 
			shared resource (the account) without any
			synchronization

				Outcome depends on the order in which accesses take place
					 This is called a race condition

			We need to ensure that only one thread at a
			time can manipulate the shared resource

				We need synchronization

		Aside: What program data is shared between threads?

			Local variables are not shared (private)
				
				Each thread has its own stack
				
				Local vars are allocated on this private stack

			Global variables and static objects are shared

				Stored in the static data segment, accessible by any thread

			Dynamic objects and other heap objs are shared

		Critical Section Requirements

			1) MutualExclusion
				if one thread is in the CS, then no other is.

			2) Progress
				If no thread is in the CS, and some threads want to enter
				CS, it should be able to enter in definite time

			3)  Bounded waiting (no starvation)

				If some thread T is waiting on the CS, then there is a limit
				on the number of times other threads can enter CS before
				this thread is granted access

			4)	Performance

				The overhead of entering and exiting the CS is small with
				respect to the work being done within it 

		Some Assumptions & Notation

			Assume no special hardware instructions, no
			restrictions on the # of processors

			Assume that basic machine language
			instructions (LOAD, STORE, etc.) are atomic:

				If two such instructions are executed concurrently, the
				result is equivalent to their sequential execution in
				some unknown order

			If only two threads, we number them T0 and T1
				 Use Ti to refer to one thread, Tj for the other (j=1-i)
				when the exact numbering doesn’t matter

		Higher-level Abstractions for CS’s

			Locks 
				Very primitive 原始, minimal semantics

			Semaphores 信号系统

				Basic, easy to understand, hard to program with

			Monitors

				High-level, ideally has language support (Java)

			Messages

				Simple model for communication & synchronization
  				
  				Direct application to distributed systems

  		Synchronization Hardware

  			To build these higher-level abstractions, it is
			useful to have some help from the hardware

			On a uniprocessor (单处理机):
				
				 Disable interrupts before entering critical section
				
				 Prevents context switches
				
				 Doesn’t work on multiprocessor

		Test-and-Set Lock (TSL):

			Test-and-set uses a lock variable
				
				Lock == 0 => nobody is using the lock
				
				Lock == 1 => lock is in use
				
				In order to acquire lock, must change it’s value from 0=>1


			semantics of TSL:

				 Record the old value of the variable
				 Set the variable to some non-zero value
				 Return the old value


			lock is always True on exit from test-and-set
				
				• Either it was True (locked) already, and nothing changed
				
				• or it was False (available), but the caller now holds it

			Return value is either True if it was locked already, or False if
			it was previously available

			A Lock Implementation:	

				There are two operation s on locks: acquire() and release()

				This is a spinlock

					Uses busy waiting - thread continually executes
					while loop in acquire() , consumes CPU cycles

WEEK 3
	
	Higher-level Abstractions for CS’s

		Semaphores

			Basic, easy to understand, hard to program with


		Producer and Consumer

			Two processes share a bounded buffer
			The producer puts info in buffer
			The consumer takes info out

		Solution
			 Sleep: Cause caller to block
			 Wakeup: Awaken a process

		Semaphores:

			Semaphores are abstract data types that
			provide synchronization.

			including:

				 An integer variable, accessed only through 2
				atomic operations
				
				 The atomic operation wait (also called P or
				decrement) - decrement the variable and block
				until semaphore is free
				
				 The atomic operation signal (also called V or
				increment) - increment the variable, unblock a
				waiting a thread if there are any
				
				 A queue of waiting threads

		Types of Semaphores
			
			 Mutex (or Binary) Semaphore
				 Represents single access to a resource
				 Guarantees mutual exclusion to a critical section
			
			 Counting semaphore
				
				 Represents a resource with many units available,
				or a resource that allows certain kinds of
				unsynchronized concurrent access (e.g., reading)
				
				 Multiple threads can pass the semaphore
				
				 Max number of threads is determined by
				semaphore’s initial value, count
					 Mutex has count = 1, counting has count = N

			Atomicity of wait() and signal()

				We must ensure that two threads cannot
				execute wait and signal at the same time

				This is another critical section problem!
					Use lower-level primitives
					 Uniprocessor: disable interrupts
					 Multiprocessor: use hardware instructions

		The readers/writers problems

			 Use three variables
				
				 Semaphore w_or_r - exclusive writing or reading
				
					 Think of it as a token that can be held either by the
					group of readers or by one individual writer.
					
					 Which thread in the group of readers is in charge of
					getting and returning the token?
					
					 “Last to leave the room turns off the light”

				 int readcount - number of threads reading object
						
					 Needed to detect when a reader is the first or last of a group.
				
				 Semaphore mutex - control access to readcount

		Monitors:

			 High-level, ideally has language support (Java)


			 Similar in a sense to an abstract data type (data and
			operations on the data) with the restriction that only one
			process at a time can be active within the monitor
			 Local data accessed only by the monitor’s procedures (not by
			any external procedure)
			 A process enters the monitor by invoking 1 of its procdures
			 Other processes that attempt to enter monitor are blocked
			 A process in the monitor may need to wait for something
			to happen
			 May need to allow another process to use the monitor
			 provide a condition type for variables with operations
			 wait (suspend the invoking process)
			 signal (resume exactly one suspended process)


		Monitor Semantics for Signal:

			 Hoare monitors
				
				 Signal() immediately switches from the caller to a
				waiting thread
				
				 Need another queue for the signaler, if signaler was not
				done using the monitor

			 Brinch Hansen
				
				 Signaler must exit monitor immediately
				
				 i.e. signal() is always the last statement in monitor
				procedure

			 Mesa monitors
				 Signal() places a waiter on the ready queue, but
				signaler continues inside monitor



	Process Scheduling

		Only one process can run at a time on a CPU
			
			 Scheduler decides which process to run
			
			 Goal of CPU scheduling:
			
			 Give illusion that processes are running concurrently
			
			 Maximize CPU utilization

		What is processor scheduling?

			 The allocation of processors to processes
			over time
			
			 This is the key to multiprogramming
				
				 We want to increase CPU utilization and job throughput
				by overlapping I/O and computation
				
				 Mechanisms:
				
					 Process states, Process queues
				
				 Policies:
					
					 Given more than one runnable process, how do we choose
					which to run next?
				
					 When do we make this decision?

		When to schedule?

			 When the running process blocks (or exits)
				 Operating system calls (e.g., I/O)

			 At fixed intervals
				 Clock interrupts
				
			 When a process enters Ready state
				 I/O interrupts, signals, process creation

		Scheduling goal

			All systems
				
				 Fairness - each process receives fair share of CPU
				
				 Avoid starvation
				
				 Policy enforcement - usage policies should be met
				
				 Balance - all parts of the system should be busy

			Batch systems
				
				 Throughput - maximize jobs completed per hour
				
				 Turnaround time - minimize time between
				submission and completion
				
				 CPU utilization - keep the CPU busy all the time

			Interactive Systems
			
				 Response time - minimize time between
				receiving request and starting to produce output
				
				 Proportionality - “simple” tasks complete quickly
			
			Real-time systems
				
				 Meet deadlines
				
				 Predictability

		Type of Scheduling

			Non-preemptive scheduling
				
				 once the CPU has been allocated to a process, it
				keeps the CPU until it terminates
				
				 Suitable for batch scheduling
			
			Preemptive scheduling
				
				 CPU can be taken from a running process and
				allocated to another
				
				 Needed in interactive or real-time systems

Week 4 Synchronization
	

	Dining philosophers: ->Semaphores

	Scheduling Algorithms	

		Non-preemptive scheduling

		FCFS

			 “First come, first served”
			
			 Non-preemptive
			
			 Choose the process at the head of the FIFO
			queue of ready processes

		Problem with FCFS	

			Average waiting time often quite long

				Convoy effect: all other processes wait for the one
				big process to release the CPU

		Shortest-Job-First



	How can we be fair, but avoid the “short jobs
	getting stuck behind long jobs” problem?

		IDEA: Allow preemption


		Preemptive scheduling


		Round Robin

			 Designed for time-sharing systems
			
			 Pre-emptive
			
			 Ready queue is circular
			
				 Each process is allowed to run for time quantum q
				before being preempted and put back on queue
			
			 Choice of quantum (aka time slice) is critical
			
				 as q  , RR  FCFS;
			
				 as q  0, RR  processor sharing (PS)
			
				 we want q to be large w.r.t. the context switch time


		Priority Scheduling

			A priority, p, is associated with each process
			
			 Highest priority job is selected from Ready
			queue
			
				 Can be pre-emptive or non-preemptive
			
			 Enforcing this policy is tricky
			
				 A low priority task may never get to run
				(starvation)
			
				 A low priority task may prevent a high priority task
				from making progress by holding a resource
				(priority inversion)

	What  doreal systems do?

		Combination of
			 Multi-level queue scheduling
				 Typically with RR and priorities
			 Feedback scheduling

	Feedback Scheduling

		Motivation:
			 Want to give priority to shorter jobs
			 Want to give priority to IO bound jobs
			 Want to give priority to interactive jobs
			...





Week 6 Memory Management
	
	Memory Management

		Every active process needs memory
		
		CPU scheduling allows processes to share
		(multiplex) the processor
		
		Must figure out how to share main memory as well

		what should our oals be?

			Support enough active processes to keep CPU busy
			
			Use memory efficiently (minimize wasted memory)
			
			Keep memory management overhead small
			
				… while satisfying basic requirements

	Requirements

		Relocation

			Programmers don’t know what physical memory will be
			available when their programs run
			 Scheduler may swap processes in/out of memory, need to be
			able to bring it back in to a different region of memory
			 This implies we will need some type of address translation


		 Logical Organization
				
				Machine accesses memory addresses as a one-dimensional
				array of bytes
				
				Programmers organize code in modules
				
				Need to map between these views

		Protection
			
			 A process’s memory should be protected from unwanted
			access by other processes, both intentional and accidental
			
			 Requires hardware support
		
		Sharing
			
			 In some instances, processes need to be able to access
			the same memory
			
			 Need ways to specify and control what sharing is allowed
		
		Physical Organization
			
			 Memory and Disk form a two-level hierarchy, flow of
			information between levels must be managed
			
			 CPU can only access data in registers or memory, not disk


		Meeting the requirements

			Modern systems use virtual memory

				Complicated technique requiring hardware & software support

			We’ll build up to virtual memory by looking at some simpler schemes first
				
				 Fixed partitioning
				 Dynamic partitioning
				 Paging
				 Segmentation

		Address Binding

			Programs must be in memory to execute

			Addresses in program must be translated to real addresses
				
				 Programmers use symbolic addresses (i.e., variable
				names) to refer to memory locations
				
				 CPU fetches from, and stores to, real memory addresses

		When are addresses bound?

			Compile time

				Called absolute code since binary contains real addresses

				Disadvantage?
					
					 Must know what memory process will use during
					
					compilation
					 No relocation is possible

			Load time
				
				 Compiler translates (binds) symbolic addresses to logical,
				relocatable addresses within compilation unit (source file)
				
				 Linker takes collection of object files and translates
				addresses to logical, absolute addresses within executable
				
					 Resolves references to symbols defined in other files/modules
				
				 Loader translates logical absolute addresses to physical
				addresses when program is loaded into memory
				
				 Disadvantage?
				
					 Programs can be loaded to different address when they start, but
					cannot be relocated later

			A better plan

				Bind addresses at execution time

					Executable object file, a.out, contains logical
					addresses for entire program
		
						 translated to a real, physical address during execution
		
						 Flexible, but requires special hardware (as we will see)

			Two key problems:
				 How do you map logical to physical addresses?
				 How do you allocate physical memory for a process?

		How to allocate physical memory?

			Fixed partitioning

				Divide memory into regions with fixed boundaries

					Can be equal-size or unequal-size

					Disadvantage?	

						Memory is wasted if process is smaller than partion(internal fragmentation)

						Programmer must deal with programs that are larger than partition
						(overlays)
				
				Equal-sized partirtions:

					 Process can be loaded into any available partition
				
				unequal-size partition:

					Queue-per-partition,assign process to smallest partition in
					which it will fit
					
						 A process always runsmin the same size of partition

					Single queue, assign process to smallest available partition

			Dynamic partition:

				Partitions vary in length and number over time
				
				 When a process is brought in to memory, a partition
				of exactly the right size is created to hold it

				Disadvantage?

					As processes come and go, “holes” are created

						Some blocks may be too small for any process

						This is called external fragmentation

					OS may move processes around to
					create larger chunks of free space

						This is called compaction
						
						Requires processes to be relocatable

			Heap Management


			Tracking Memory Allocation

				Bitmaps
					
					1 bit per allocation unit
					
					“0” == free, “1” == allocated

				Advantage/Disadvantages?
					
					 Allocating a N-unit chunk requires scanning
					bitmap for sequence of N zero’s
					
					 Slow 

				Free lists
					 Maintain linked list of allocated and free segments
					 List needs memory too. Where do we store it?



				Implicit list
					
					 Each block has header that records size and status
					(allocated or free)
					
					 Searching for free block is linear in total number of blocks


				Explicit list 

					Store pointers in free blocks to create doubly-linked list

			Placement Algorithms

				Compaction is time-consuming and not always possible
				
				We can reduce the need for it by being careful about
				how memory is allocated to processes over time
				
				Given multiple blocks of free memory of sufficient size,
				how should we choose which one to use?

				First-fit - choose first block that is large enough; search can
				start at beginning, or where previous search ended (called
				next-fit)

					Simplest, and often fastest and most efficient
					 May leave many small fragments near start of memory that
					must be searched repeatedly
				
				Best-fit - choose the block that is closest in size to the request
					
					left-over fragments tend to be small (unusable)
					 In practice, similar storage utilization to first-fit

				Worst-fit – choose the largest block

					Not as good as best-fit or first-fit in practice
				
				Quick-fit – keep multiple free lists for common block sizes

					Great for fast allocation, generally harder to coalesce

		Problems with Partitions:

			With fixed partitioning, internal fragmentation and
			need for overlays are big problems

			With dynamic partitioning, external fragmentation
			and management of space are major problems

			Basic problem is that processes must be allocated
			to contiguous blocks of physical memory

		Paging

			Partition memory into equal, fixed-size chunks
				
				 These are called page frames or simply frames
			
			 Divide processes’ memory into chunks of the same size
			
				 These are called pages
			
			 Possible page frame sizes are restricted to
			powers of 2 to simplify translation

			Is there fragmentation with paging?

				External fragmentation is eliminated
				
				Internal fragmentation is at most a part of one
				page per process

		Address translation

			Swapping and compaction require a way to
			change the physical memory addresses a
			process refers to


			Really, need dynamic relocation (aka
			execution-time binding of addresses)

		Address translationfor Paging:	

			Need more than base & limit registers now
			
			Operating system maintains a page table for each
			process

		Support for paging

			Oerating system maintains page table for each process

				Page table records which physical frame holds each page

				virtual addresses are now page number + page offset

					 page number = ?
						
						 =vaddr / page_size
					
					 page offset = ?
						
						 vaddr % page_size
						
						log2(page_size in bytes)


			Page Table Entries (PTE)

				1 1 1 3    26
				M R V Prot Page Frame Number

				Page table entries (PTEs) control mapping
					
					 Modify bit (M) says whether or not page has been written
					
						 Set when a write to a page occurs
					
					 Reference bit (R) says whether page has been accessed
						
						 Set when a read or write to the page occurs
					
					 Valid bit (V) says whether PTE can be used
						
						 Checked on each use of virtual address
					
					 Protection bits specify what operations are allowed on page
					
						 Read/write/execute
					
					 Page frame number (PFN) determines physical page
					
					 Not all bits are provided by all architectures

			TLBs

				Translate virtual page #s into PTEs

















































